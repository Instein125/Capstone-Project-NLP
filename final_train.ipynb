{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU5Z6UsP5vnl",
        "outputId": "29d499e2-a4b1-4e5c-d19a-0279e42a8f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "x_zip_path = \"/content/drive/MyDrive/Colab Notebooks/speech denoiser/x_train_noised_speech.zip\"\n",
        "y_zip_path = \"/content/drive/MyDrive/Colab Notebooks/speech denoiser/y_train_clean_audio.zip\"\n",
        "\n",
        "with zipfile.ZipFile(x_zip_path, 'r') as x_zip_ref, zipfile.ZipFile(y_zip_path, 'r') as y_zip_ref:\n",
        "    x_file_list = [f for f in x_zip_ref.namelist() if f.endswith(\".npy\")]\n",
        "    y_file_list = [f for f in y_zip_ref.namelist() if f.endswith(\".npy\")]\n",
        "\n",
        "# Ensure that the lists are sorted to match X and Y pairs\n",
        "x_file_list.sort()\n",
        "y_file_list.sort()"
      ],
      "metadata": {
        "id": "soxEb18C6hG3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdftiemy7JMl",
        "outputId": "7e377ecb-d738-4c0e-96ce-6e6c6c6bd685"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10679"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_spectrogram(spectrogram, final_shape):\n",
        "  # Pad the spectrogram to match the desired final shape\n",
        "  if spectrogram.shape[1] < final_shape[1]:\n",
        "    # Pad the spectrogram to match the desired final shape\n",
        "    pad_width = ((0, 0), (0, final_shape[1] - spectrogram.shape[1]))\n",
        "    processed_spectrogram = np.pad(spectrogram, pad_width, mode='constant', constant_values=0)\n",
        "  elif spectrogram.shape[1] > final_shape[1]:\n",
        "    # Trim the spectrogram to match the desired final shape\n",
        "    processed_spectrogram = spectrogram[:final_shape[0], :final_shape[1]]\n",
        "  else:\n",
        "    processed_spectrogram = spectrogram  # No change needed if the shape is already as desired\n",
        "  # Append the padded spectrogram to the x_train list\n",
        "  processed_spectrogram = processed_spectrogram[:final_shape[0], :]\n",
        "\n",
        "  return processed_spectrogram\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K8lHdnuP8LFG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as k\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "class UNET:\n",
        "    \"\"\"\n",
        "    Unet represents a Deep Convolutional encoder decoder architecture\n",
        "    with skip connection.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_shape,\n",
        "                 conv_filters,\n",
        "                 conv_kernels,):\n",
        "        self.input_shape = input_shape # [256, 256, 1]\n",
        "        self.conv_filters = conv_filters # [64, 1280, 256, 512]\n",
        "        self.conv_kernels = conv_kernels # (3, 3)\n",
        "\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "        self._model_input = None\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def summary(self):\n",
        "        self.model.summary()\n",
        "\n",
        "    def compile(self, learning_rate=0.001):\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        binary_loss = tf.keras.losses.BinaryCrossentropy()\n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                           loss=binary_loss,\n",
        "                           metrics=[self.iou,])\n",
        "\n",
        "    def train(self, x_train, y_train, batch_size, num_epochs, callbacks):\n",
        "        history=self.model.fit(x_train,\n",
        "                       y_train,\n",
        "                       batch_size=batch_size,\n",
        "                       epochs=num_epochs,\n",
        "                       callbacks=callbacks,\n",
        "                       validation_split=0.2,\n",
        "                       )\n",
        "        return history\n",
        "\n",
        "    def train_generator(self, train_generator, valid_generator, batch_size, num_epochs, callbacks):\n",
        "        history=self.model.fit(\n",
        "                      train_generator,\n",
        "                      batch_size = batch_size,\n",
        "                      epochs=num_epochs,\n",
        "                      callbacks=callbacks,\n",
        "                      validation_data = valid_generator,\n",
        "                       )\n",
        "        return history\n",
        "\n",
        "    def save(self, save_folder):\n",
        "        self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "        self._save_parameters(save_folder)\n",
        "        self._save_weights(save_folder)\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        self.model.load_weights(weights_path)\n",
        "\n",
        "    # For predicitons\n",
        "    def reconstruct(self, spectrograms):\n",
        "        predictions = self.model.predict(spectrograms)\n",
        "        return predictions\n",
        "\n",
        "    # Plots the graph for training and validation loss and iou score\n",
        "    def plot_loss(self, history):\n",
        "        plt.plot(history.history['loss'],label=\"loss\")\n",
        "        plt.plot(history.history['val_loss'],label=\"validation loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_iou(self, history):\n",
        "        plt.plot(history.history['iou'],label=\"iou\")\n",
        "        plt.plot(history.history['val_iou'],label=\"validation iou\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    # Calculates the Iou score matrics\n",
        "    def iou(self, y_true, y_pred, smooth = 1):\n",
        "        y_true = k.flatten(y_true)\n",
        "        y_pred = k.flatten(y_pred)\n",
        "        intersection = k.sum(y_true*y_pred)\n",
        "        union = k.sum(y_true)+k.sum(y_pred)-intersection\n",
        "        iou_score = (intersection+smooth)/(union+smooth)\n",
        "        return iou_score\n",
        "\n",
        "    def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "\n",
        "    def _save_parameters(self, save_folder):\n",
        "        parameters = [\n",
        "            self.input_shape,\n",
        "            self.conv_filters,\n",
        "            self.conv_kernels,\n",
        "        ]\n",
        "        save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            pickle.dump(parameters, f)\n",
        "\n",
        "    def _save_weights(self, save_folder):\n",
        "        save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        self.model.save_weights(save_path)\n",
        "\n",
        "    def _build(self):\n",
        "        self._build_unet()\n",
        "\n",
        "    # Define the U-Net architecture\n",
        "    def _build_unet(self):\n",
        "        inputs = tf.keras.Input(self.input_shape)\n",
        "\n",
        "        conv1, pool1 = self._downsample_block(inputs, 64, (3, 3))\n",
        "        conv2, pool2 = self._downsample_block(pool1, 128, (3, 3))\n",
        "        conv3, pool3 = self._downsample_block(pool2, 256, (3, 3))\n",
        "        conv4, pool4 = self._downsample_block(pool3, 512, (3, 3))\n",
        "\n",
        "        conv5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)\n",
        "        conv5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(conv5)\n",
        "\n",
        "        conv6 = self._upsample_block(conv5, conv4, 512, (3, 3))\n",
        "        conv7 = self._upsample_block(conv6, conv3, 256, (3, 3))\n",
        "        conv8 = self._upsample_block(conv7, conv2, 128, (3, 3))\n",
        "        conv9 = self._upsample_block(conv8, conv1, 64, (3, 3))\n",
        "\n",
        "        outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "        self.model = models.Model(inputs=inputs, outputs=outputs, name = 'Unet')\n",
        "\n",
        "    # Downsampling block (encoder)\n",
        "    def _downsample_block(self, input_layer, filters, kernel_size, padding='same', activation='relu'):\n",
        "        conv1 = layers.Conv2D(filters, kernel_size, activation=activation, padding=padding)(input_layer)\n",
        "        conv1 = layers.Dropout(0.1)(conv1)\n",
        "        conv1 = layers.Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv1)\n",
        "        b1 = layers.BatchNormalization()(conv1)\n",
        "        r1 = layers.ReLU()(b1)\n",
        "        pool = layers.MaxPooling2D(pool_size=(2, 2))(r1)\n",
        "        print(\"Downsample block shape is: \" ,conv1.shape)\n",
        "        return conv1, pool\n",
        "\n",
        "    # Upsampling block\n",
        "    def _upsample_block(self, input_layer, skip_connection, filters, kernel_size, padding='same', activation='relu'):\n",
        "        up = layers.UpSampling2D(size=(2, 2))(input_layer)\n",
        "        up = layers.Conv2DTranspose(filters, kernel_size, activation=activation, padding=padding)(up)\n",
        "        merge = layers.concatenate([up, skip_connection], axis=3)\n",
        "        conv = layers.Conv2D(filters, 3, activation=activation, padding=padding)(merge)\n",
        "        conv = layers.Conv2D(filters, 3, activation=activation, padding=padding)(conv)\n",
        "        print(\"upsample block shape is: \" ,conv.shape)\n",
        "        return conv\n",
        ""
      ],
      "metadata": {
        "id": "3YXXsEFkBxgA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "def data_generator(x_zip_path, y_zip_path, final_shape, batch_size=32):\n",
        "    with zipfile.ZipFile(x_zip_path, 'r') as x_zip_ref, zipfile.ZipFile(y_zip_path, 'r') as y_zip_ref:\n",
        "        x_file_list = [f for f in x_zip_ref.namelist() if f.endswith(\".npy\")]\n",
        "        y_file_list = [f for f in y_zip_ref.namelist() if f.endswith(\".npy\")]\n",
        "\n",
        "        # Ensure that the lists are sorted to match X and Y pairs\n",
        "        x_file_list.sort()\n",
        "        y_file_list.sort()\n",
        "\n",
        "        # Check if the number of files in both folders match\n",
        "        assert len(x_file_list) == len(y_file_list), \"Mismatch in the number of files in X and Y folders\"\n",
        "\n",
        "    while True:\n",
        "        for batch_start in range(0, len(x_file_list), batch_size):\n",
        "            batch_x_files = x_file_list[batch_start:batch_start + batch_size]\n",
        "            batch_y_files = y_file_list[batch_start:batch_start + batch_size]\n",
        "            batch_x_data = []\n",
        "            batch_y_data = []\n",
        "\n",
        "            for x_filename, y_filename in zip(batch_x_files, batch_y_files):\n",
        "                with zipfile.ZipFile(x_zip_path, 'r') as x_zip_ref:\n",
        "                    with x_zip_ref.open(x_filename) as x_file:\n",
        "                        x_spectrogram = np.load(x_file)\n",
        "\n",
        "                with zipfile.ZipFile(y_zip_path, 'r') as y_zip_ref:\n",
        "                    with y_zip_ref.open(y_filename) as y_file:\n",
        "                        y_spectrogram = np.load(y_file)\n",
        "\n",
        "                processed_x_spectrogram = process_spectrogram(x_spectrogram, final_shape)\n",
        "                processed_y_spectrogram = process_spectrogram(y_spectrogram, final_shape)\n",
        "\n",
        "                batch_x_data.append(processed_x_spectrogram)\n",
        "                batch_y_data.append(processed_y_spectrogram)\n",
        "\n",
        "\n",
        "            batch_x_data = np.array(batch_x_data)\n",
        "            batch_y_data = np.array(batch_y_data)\n",
        "\n",
        "            batch_x_data = batch_x_data.reshape(batch_x_data.shape[0], batch_x_data.shape[1], batch_x_data.shape[2], 1 )\n",
        "            batch_y_data = batch_y_data.reshape(batch_y_data.shape[0], batch_y_data.shape[1], batch_y_data.shape[2], 1 )\n",
        "            yield batch_x_data, batch_y_data\n",
        "\n",
        "# Example usage\n",
        "x_zip_path = \"/content/drive/MyDrive/Colab Notebooks/speech denoiser/x_train_noised_speech.zip\"\n",
        "y_zip_path = \"/content/drive/MyDrive/Colab Notebooks/speech denoiser/y_train_clean_audio.zip\"\n"
      ],
      "metadata": {
        "id": "NO9c7xSZ57pG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNET(\n",
        "        input_shape=(256, 512, 1),\n",
        "        conv_filters=(64, 128, 256, 512),\n",
        "        conv_kernels=(3, 3),\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJonz_SA9ca9",
        "outputId": "270235a5-3445-4833-9968-d669229aa23d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downsample block shape is:  (None, 256, 512, 64)\n",
            "Downsample block shape is:  (None, 128, 256, 128)\n",
            "Downsample block shape is:  (None, 64, 128, 256)\n",
            "Downsample block shape is:  (None, 32, 64, 512)\n",
            "upsample block shape is:  (None, 32, 64, 512)\n",
            "upsample block shape is:  (None, 64, 128, 256)\n",
            "upsample block shape is:  (None, 128, 256, 128)\n",
            "upsample block shape is:  (None, 256, 512, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile()"
      ],
      "metadata": {
        "id": "dayfAqF9_XKY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from callback import callbackList\n",
        "callbacks = callbackList()"
      ],
      "metadata": {
        "id": "s68Py7Gg-cRG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (256, 512)\n",
        "batch_size = 8\n",
        "train_generator = data_generator(x_zip_path=x_zip_path, y_zip_path=y_zip_path, final_shape=shape, batch_size=batch_size)\n",
        "valid_generator = data_generator(x_zip_path=x_zip_path, y_zip_path=y_zip_path, final_shape=shape, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "oyMQy7xpEPGG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.train_generator(train_generator, valid_generator,batch_size= batch_size, num_epochs = 2, callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "Fgk-ZK-t9oLj",
        "outputId": "bfff6c5f-bf1f-4a2c-f032-51bbf742b933"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "    140/Unknown - 308s 2s/step - loss: 0.3359 - iou: 0.2351"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8cbab7d05b3e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-88665d3f29b9>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(self, train_generator, valid_generator, batch_size, num_epochs, callbacks)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         history=self.model.fit(\n\u001b[0m\u001b[1;32m     51\u001b[0m                       \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                       \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOV1WIV4-sL4",
        "outputId": "ab05e7c1-0547-468d-d495-787f46e9c6b8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 17 16:32:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0    31W /  70W |  14571MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUQqDtSyH3hl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}